hyper_params:
  # learning rate
  lr: 0.005
  # type of tokenizer to use
  tokenizer: "basic_english"
  # size of the embedding space
  embed_size: 128
  # number of decoder layers running in parallel
  num_layers: 3
  # dropout
  dropout: 0.2
  # number of tokens in each time step
  block_size: 64
  # number of self-attention heads
  num_heads: 8

train_params:
  # batch size (use a higher number if memory is big)
  batch_size: 32
  # how many training cycles to run
  num_epochs: 25